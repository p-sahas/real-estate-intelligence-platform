{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583ee79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Packages ready\n"
     ]
    }
   ],
   "source": [
    "#  Setup & Installations\n",
    "import sys\n",
    "\n",
    "# if \"google.colab\" in sys.modules or True:\n",
    "#     print(\" Installing required packages...\")\n",
    "#     %pip install -q langchain>=0.1.0 langchain-openai>=0.0.5 langchain-community>=0.0.20 langchain-text-splitters>=0.2.0 chromadb>=0.4.0 tiktoken>=0.5.0 python-dotenv>=1.0.0\n",
    "\n",
    "print(\" Packages ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7470a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Environment loaded\n",
      " Provider: OpenRouter\n",
      " Project root: c:\\Development\\real-estate-intelligence-platform\n"
     ]
    }
   ],
   "source": [
    "#  Imports & Environment Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# Load environment\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "# Check for API key (OpenRouter preferred, OpenAI as fallback)\n",
    "openrouter_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openrouter_key and not openai_key:\n",
    "    raise EnvironmentError(\n",
    "        \"   No API key found!\\n\"\n",
    "        \"   Add OPENROUTER_API_KEY (recommended) or OPENAI_API_KEY to .env\"\n",
    "    )\n",
    "\n",
    "# Load configuration\n",
    "from context_engineering.config import (\n",
    "    CRAWL_OUT_DIR, VECTOR_DIR, EMBEDDING_MODEL, PROVIDER\n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "provider = \"OpenRouter\" if openrouter_key else \"OpenAI\"\n",
    "print(\" Environment loaded\")\n",
    "print(f\" Provider: {provider}\")\n",
    "print(f\" Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba9083",
   "metadata": {},
   "source": [
    "### Import Chunking Services\n",
    "\n",
    "Using chunking functions from application layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a4832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chunking services loaded from service layer\n",
      " Location: context_engineering.application.ingest_documents_service.chunkers\n",
      "\n",
      " Available strategies:\n",
      "   1. semantic_chunk  - Split by heading structure\n",
      "   2. fixed_chunk     - Uniform 800-token chunks with overlap\n",
      "   3. sliding_chunk   - Overlapping windows for better recall\n",
      "   4. parent_child_chunk - Chunk with parent-child relationships\n",
      "   5. late_chunk_index - Chunk with late indexing\n"
     ]
    }
   ],
   "source": [
    "#  Import Chunking Services\n",
    "from context_engineering.application.ingest_documents_service import (\n",
    "    semantic_chunk,\n",
    "    fixed_chunk,\n",
    "    sliding_chunk, \n",
    "    parent_child_chunk,\n",
    "    late_chunk_index\n",
    ")\n",
    "\n",
    "print(\" Chunking services loaded from service layer\")\n",
    "print(\" Location: context_engineering.application.ingest_documents_service.chunkers\")\n",
    "print(\"\\n Available strategies:\")\n",
    "print(\"   1. semantic_chunk  - Split by heading structure\")\n",
    "print(\"   2. fixed_chunk     - Uniform 800-token chunks with overlap\")\n",
    "print(\"   3. sliding_chunk   - Overlapping windows for better recall\")\n",
    "print(\"   4. parent_child_chunk - Chunk with parent-child relationships\")\n",
    "print(\"   5. late_chunk_index - Chunk with late indexing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a60bff",
   "metadata": {},
   "source": [
    "###  Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4d3bfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 424 documents\n",
      " Total content size: 256,340 chars\n"
     ]
    }
   ],
   "source": [
    "jsonl_path = CRAWL_OUT_DIR / \"primelands_docs.jsonl\"\n",
    "\n",
    "if not jsonl_path.exists():\n",
    "    raise FileNotFoundError(f\" Corpus not found. Run 01_crawl_primelands.ipynb first.\")\n",
    "\n",
    "with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "    documents = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\" Loaded {len(documents)} documents\")\n",
    "print(f\" Total content size: {sum(len(d['content']) for d in documents):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a52461",
   "metadata": {},
   "source": [
    "### Apply Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb21ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attempting to clean: c:\\Development\\real-estate-intelligence-platform\\data\\vectorstore\n",
      "    Cleaned up successfully\n",
      " Fresh vector directory ready: c:\\Development\\real-estate-intelligence-platform\\data\\vectorstore\n"
     ]
    }
   ],
   "source": [
    "# Cleanup Vector Store (prevents corruption)\n",
    "import shutil\n",
    "import os\n",
    "import stat\n",
    "import time\n",
    "\n",
    "def on_rm_error(func, path, exc_info):\n",
    "    # Error handler for shutil.rmtree\n",
    "    try:\n",
    "        os.chmod(path, stat.S_IWRITE)\n",
    "        func(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Try to remove existing vector store\n",
    "LOCK_DETECTED = False\n",
    "if VECTOR_DIR.exists():\n",
    "    print(f\" Attempting to clean: {VECTOR_DIR}\")\n",
    "    try:\n",
    "        shutil.rmtree(VECTOR_DIR, onerror=on_rm_error)\n",
    "        print(\"    Cleaned up successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Cleanup failed ({e})\")\n",
    "        # Try renaming as last resort cleanup\n",
    "        try:\n",
    "             backup = VECTOR_DIR.with_name(f\"vectorstore_locked_{int(time.time())}\")\n",
    "             os.rename(VECTOR_DIR, backup)\n",
    "             print(f\"    Renamed locked dir to: {backup}\")\n",
    "        except Exception as e2:\n",
    "             LOCK_DETECTED = True\n",
    "             print(f\"    CRITICAL LOCK: Could not delete or rename ({e2})\")\n",
    "\n",
    "if LOCK_DETECTED:\n",
    "    # OVERRIDE VECTOR_DIR to use a fresh path\n",
    "    print(\"\\n  FILE LOCK DETECTED (likely opened in editor)\")\n",
    "    print(\"    Switching to a new directory to bypass lock...\")\n",
    "    VECTOR_DIR = VECTOR_DIR.with_name(\"vectorstore_v2\")\n",
    "    print(f\"    NEW TARGET: {VECTOR_DIR}\")\n",
    "else:\n",
    "    # Create fresh standard directory\n",
    "    VECTOR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\" Fresh vector directory ready: {VECTOR_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66251a36",
   "metadata": {},
   "source": [
    "#### 01. Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "024c75ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running semantic chunking...\n",
      " Semantic chunking complete: 414 chunks\n",
      " Saved to: c:\\Development\\real-estate-intelligence-platform\\data\\chunks_semantic.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\" Running semantic chunking...\")\n",
    "semantic_chunks = semantic_chunk(documents)\n",
    "\n",
    "# Save\n",
    "semantic_path = CRAWL_OUT_DIR / \"chunks_semantic.jsonl\"\n",
    "with open(semantic_path, 'w', encoding='utf-8') as f:\n",
    "    for chunk in semantic_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\" Semantic chunking complete: {len(semantic_chunks)} chunks\")\n",
    "print(f\" Saved to: {semantic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250ae60",
   "metadata": {},
   "source": [
    "#### 02. Fixed-Window Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aee6ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running fixed-window chunking...\n",
      " Fixed chunking complete: 424 chunks\n",
      " Avg token count: 213.5\n",
      " Saved to: c:\\Development\\real-estate-intelligence-platform\\data\\chunks_fixed.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\" Running fixed-window chunking...\")\n",
    "fixed_chunks = fixed_chunk(documents)\n",
    "\n",
    "# Save\n",
    "fixed_path = CRAWL_OUT_DIR / \"chunks_fixed.jsonl\"\n",
    "with open(fixed_path, 'w', encoding='utf-8') as f:\n",
    "    for chunk in fixed_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "avg_tokens = sum(c['token_count'] for c in fixed_chunks) / len(fixed_chunks) if fixed_chunks else 0\n",
    "print(f\" Fixed chunking complete: {len(fixed_chunks)} chunks\")\n",
    "print(f\" Avg token count: {avg_tokens:.1f}\")\n",
    "print(f\" Saved to: {fixed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f5418",
   "metadata": {},
   "source": [
    "#### 03. Sliding-Window Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1bb931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running sliding-window chunking...\n",
      " Sliding chunking complete: 424 chunks\n",
      " Saved to: c:\\Development\\real-estate-intelligence-platform\\data\\chunks_sliding.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\" Running sliding-window chunking...\")\n",
    "sliding_chunks = sliding_chunk(documents)\n",
    "\n",
    "# Save\n",
    "sliding_path = CRAWL_OUT_DIR / \"chunks_sliding.jsonl\"\n",
    "with open(sliding_path, 'w', encoding='utf-8') as f:\n",
    "    for chunk in sliding_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\" Sliding chunking complete: {len(sliding_chunks)} chunks\")\n",
    "print(f\" Saved to: {sliding_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59befc54",
   "metadata": {},
   "source": [
    "#### 04. Parent-Child Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "787ded37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running Parent Child Chunking...\n",
      " Parent child chunking complete: 848 chunks\n",
      " Avg token count: 213.5\n",
      " Saved to: c:\\Development\\real-estate-intelligence-platform\\data\\chunks_parent_child.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\" Running Parent Child Chunking...\")\n",
    "child_chunks, parent_chunks = parent_child_chunk(documents)\n",
    "parent_child_chunks = child_chunks + parent_chunks\n",
    "\n",
    "# Save\n",
    "parent_child_path = CRAWL_OUT_DIR / \"chunks_parent_child.jsonl\"\n",
    "with open(parent_child_path, 'w', encoding='utf-8') as f:\n",
    "    for chunk in parent_child_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "avg_tokens = sum(c['token_count'] for c in parent_child_chunks) / len(parent_child_chunks) if parent_child_chunks else 0\n",
    "print(f\" Parent child chunking complete: {len(parent_child_chunks)} chunks\")\n",
    "print(f\" Avg token count: {avg_tokens:.1f}\")\n",
    "print(f\" Saved to: {parent_child_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128ea8e",
   "metadata": {},
   "source": [
    "#### 05. Late Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a86b6e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running Late Chunking...\n",
      "\n",
      " Late chunking complete: 424 base passages\n",
      " Avg token count: 213.5\n",
      " Saved to: c:\\Development\\real-estate-intelligence-platform\\data\\chunks_late.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" Running Late Chunking...\\n\")\n",
    "late_chunks = late_chunk_index(documents)\n",
    "\n",
    "# Save\n",
    "late_chunks_path = CRAWL_OUT_DIR / \"chunks_late.jsonl\"\n",
    "with open(late_chunks_path, 'w', encoding='utf-8') as f:\n",
    "    for chunk in late_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "avg_tokens = sum(c.get('token_count', 0) for c in late_chunks) / len(late_chunks) if late_chunks else 0\n",
    "print(f\" Late chunking complete: {len(late_chunks)} base passages\")\n",
    "print(f\" Avg token count: {avg_tokens:.1f}\")\n",
    "print(f\" Saved to: {late_chunks_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75b553",
   "metadata": {},
   "source": [
    "#### Spot-Check Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b10d450b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spot-Check: 2 samples from each strategy\n",
      "\n",
      "============================================================\n",
      "SEMANTIC SAMPLES\n",
      "============================================================\n",
      "**Semantic** chunk:\n",
      "  URL: https://www.primelands.lk/land/GREEN-CLASSIC-WADDUWA/en\n",
      "  Strategy: semantic\n",
      "  Text length: 614 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "**Semantic** chunk:\n",
      "  URL: https://www.primelands.lk/land/FELICITY-MINUWANGODA/en\n",
      "  Strategy: semantic\n",
      "  Text length: 612 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "============================================================\n",
      "FIXED-WINDOW SAMPLES\n",
      "============================================================\n",
      "**Fixed** chunk:\n",
      "  URL: https://www.primelands.lk/land/NILLAMBA-SATURDAY/en\n",
      "  Strategy: fixed\n",
      "  Text length: 600 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "**Fixed** chunk:\n",
      "  URL: https://www.primelands.lk/news/88/Prime-Group-appoints-international-music-icon-Umaria-Sinhawansa-as-Global-Brand-Ambassador/en\n",
      "  Strategy: fixed\n",
      "  Text length: 752 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "============================================================\n",
      "SLIDING-WINDOW SAMPLES\n",
      "============================================================\n",
      "**Sliding** chunk:\n",
      "  URL: https://www.primelands.lk/land/district/Galle/en\n",
      "  Strategy: sliding\n",
      "  Text length: 594 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "**Sliding** chunk:\n",
      "  URL: https://www.primelands.lk/land/city/Moratuwa/en\n",
      "  Strategy: sliding\n",
      "  Text length: 592 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "============================================================\n",
      "PARENT-CHILD SAMPLES\n",
      "============================================================\n",
      "**Parent-Child** chunk:\n",
      "  URL: https://www.primelands.lk/apartment/ONE-TANGALLE-TANGALLE/en\n",
      "  Strategy: parent\n",
      "  Text length: 618 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "**Parent-Child** chunk:\n",
      "  URL: https://www.primelands.lk/land/NEGOMBO-CITY-BASE/en\n",
      "  Strategy: child\n",
      "  Text length: 600 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "============================================================\n",
      "LATE-CHUNKING SAMPLES\n",
      "============================================================\n",
      "**Late-Chunking** chunk:\n",
      "  URL: https://www.primelands.lk/house/GAMPAHA-ELEMINT-SUITES/en\n",
      "  Strategy: late_chunk_base\n",
      "  Text length: 612 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "**Late-Chunking** chunk:\n",
      "  URL: https://www.primelands.lk/news/en\n",
      "  Strategy: late_chunk_base\n",
      "  Text length: 564 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" Spot-Check: 2 samples from each strategy\\n\")\n",
    "\n",
    "def print_sample(chunk, strategy_name):\n",
    "    print(f\"**{strategy_name}** chunk:\")\n",
    "    print(f\"  URL: {chunk['url']}\")\n",
    "    print(f\"  Strategy: {chunk['strategy']}\")\n",
    "    print(f\"  Text length: {len(chunk['text'])} chars\")\n",
    "    print(f\"  Preview: {chunk['text'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SEMANTIC SAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for chunk in random.sample(semantic_chunks, min(2, len(semantic_chunks))):\n",
    "    print_sample(chunk, \"Semantic\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FIXED-WINDOW SAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for chunk in random.sample(fixed_chunks, min(2, len(fixed_chunks))):\n",
    "    print_sample(chunk, \"Fixed\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SLIDING-WINDOW SAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for chunk in random.sample(sliding_chunks, min(2, len(sliding_chunks))):\n",
    "    print_sample(chunk, \"Sliding\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PARENT-CHILD SAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for chunk in random.sample(parent_child_chunks, min(2, len(child_chunks))):\n",
    "    print_sample(chunk, \"Parent-Child\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LATE-CHUNKING SAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for chunk in random.sample(late_chunks, min(2, len(late_chunks))):\n",
    "    print_sample(chunk, \"Late-Chunking\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sahas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
