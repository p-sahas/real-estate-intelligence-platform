{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583ee79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Packages ready\n"
     ]
    }
   ],
   "source": [
    "#  Setup & Installations\n",
    "import sys\n",
    "\n",
    "# if \"google.colab\" in sys.modules or True:\n",
    "#     print(\" Installing required packages...\")\n",
    "#     %pip install -q langchain>=0.1.0 langchain-openai>=0.0.5 langchain-community>=0.0.20 langchain-text-splitters>=0.2.0 chromadb>=0.4.0 tiktoken>=0.5.0 python-dotenv>=1.0.0\n",
    "\n",
    "print(\" Packages ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7470a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Environment loaded\n",
      " Provider: OpenRouter\n",
      " Project root: c:\\Development\\real-estate-intelligence-platform\n"
     ]
    }
   ],
   "source": [
    "#  Imports & Environment Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "# Load environment\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "# Check for API key (OpenRouter preferred, OpenAI as fallback)\n",
    "openrouter_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openrouter_key and not openai_key:\n",
    "    raise EnvironmentError(\n",
    "        \"   No API key found!\\n\"\n",
    "        \"   Add OPENROUTER_API_KEY (recommended) or OPENAI_API_KEY to .env\"\n",
    "    )\n",
    "\n",
    "# Load configuration\n",
    "from context_engineering.config import (\n",
    "    CRAWL_OUT_DIR, VECTOR_DIR, EMBEDDING_MODEL, PROVIDER\n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "provider = \"OpenRouter\" if openrouter_key else \"OpenAI\"\n",
    "print(\" Environment loaded\")\n",
    "print(f\" Provider: {provider}\")\n",
    "print(f\" Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ba9083",
   "metadata": {},
   "source": [
    "### Import Chunking Services\n",
    "\n",
    "Using chunking functions from application layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a4832e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chunking services loaded from service layer\n",
      " Location: context_engineering.application.ingest_documents_service.chunkers\n",
      "\n",
      " Available strategies:\n",
      "   1. semantic_chunk  - Split by heading structure\n",
      "   2. fixed_chunk     - Uniform 800-token chunks with overlap\n",
      "   3. sliding_chunk   - Overlapping windows for better recall\n",
      "   4. parent_child_chunk - Chunk with parent-child relationships\n",
      "   5. late_chunk_index - Chunk with late indexing\n"
     ]
    }
   ],
   "source": [
    "#  Import Chunking Services\n",
    "from context_engineering.application.ingest_documents_service import (\n",
    "    semantic_chunk,\n",
    "    fixed_chunk,\n",
    "    sliding_chunk, \n",
    "    parent_child_chunk,\n",
    "    late_chunk_index\n",
    ")\n",
    "\n",
    "print(\" Chunking services loaded from service layer\")\n",
    "print(\" Location: context_engineering.application.ingest_documents_service.chunkers\")\n",
    "print(\"\\n Available strategies:\")\n",
    "print(\"   1. semantic_chunk  - Split by heading structure\")\n",
    "print(\"   2. fixed_chunk     - Uniform 800-token chunks with overlap\")\n",
    "print(\"   3. sliding_chunk   - Overlapping windows for better recall\")\n",
    "print(\"   4. parent_child_chunk - Chunk with parent-child relationships\")\n",
    "print(\"   5. late_chunk_index - Chunk with late indexing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a60bff",
   "metadata": {},
   "source": [
    "###  Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4d3bfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 424 documents\n",
      " Total content size: 256,340 chars\n"
     ]
    }
   ],
   "source": [
    "jsonl_path = CRAWL_OUT_DIR / \"primelands_docs.jsonl\"\n",
    "\n",
    "if not jsonl_path.exists():\n",
    "    raise FileNotFoundError(f\" Corpus not found. Run 01_crawl_primelands.ipynb first.\")\n",
    "\n",
    "with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "    documents = [json.loads(line) for line in f]\n",
    "\n",
    "print(f\" Loaded {len(documents)} documents\")\n",
    "print(f\" Total content size: {sum(len(d['content']) for d in documents):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a52461",
   "metadata": {},
   "source": [
    "### Apply Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb21ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attempting to clean: c:\\Development\\real-estate-intelligence-platform\\data\\vectorstore\n",
      "    Cleaned up successfully\n",
      " Fresh vector directory ready: c:\\Development\\real-estate-intelligence-platform\\data\\vectorstore\n"
     ]
    }
   ],
   "source": [
    "# Cleanup Vector Store (prevents corruption)\n",
    "import shutil\n",
    "import os\n",
    "import stat\n",
    "import time\n",
    "\n",
    "def on_rm_error(func, path, exc_info):\n",
    "    # Error handler for shutil.rmtree\n",
    "    try:\n",
    "        os.chmod(path, stat.S_IWRITE)\n",
    "        func(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Try to remove existing vector store\n",
    "LOCK_DETECTED = False\n",
    "if VECTOR_DIR.exists():\n",
    "    print(f\" Attempting to clean: {VECTOR_DIR}\")\n",
    "    try:\n",
    "        shutil.rmtree(VECTOR_DIR, onerror=on_rm_error)\n",
    "        print(\"    Cleaned up successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Cleanup failed ({e})\")\n",
    "        # Try renaming as last resort cleanup\n",
    "        try:\n",
    "             backup = VECTOR_DIR.with_name(f\"vectorstore_locked_{int(time.time())}\")\n",
    "             os.rename(VECTOR_DIR, backup)\n",
    "             print(f\"    Renamed locked dir to: {backup}\")\n",
    "        except Exception as e2:\n",
    "             LOCK_DETECTED = True\n",
    "             print(f\"    CRITICAL LOCK: Could not delete or rename ({e2})\")\n",
    "\n",
    "if LOCK_DETECTED:\n",
    "    # OVERRIDE VECTOR_DIR to use a fresh path\n",
    "    print(\"\\n  FILE LOCK DETECTED (likely opened in editor)\")\n",
    "    print(\"    Switching to a new directory to bypass lock...\")\n",
    "    VECTOR_DIR = VECTOR_DIR.with_name(\"vectorstore_v2\")\n",
    "    print(f\"    NEW TARGET: {VECTOR_DIR}\")\n",
    "else:\n",
    "    # Create fresh standard directory\n",
    "    VECTOR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\" Fresh vector directory ready: {VECTOR_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66251a36",
   "metadata": {},
   "source": [
    "#### 01. Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "024c75ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running semantic chunking...\n",
      " Semantic chunking complete: 414 chunks\n",
      " Saved to: c:\\Development\\real-estate-intelligence-platform\\data\\chunks_semantic.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\" Running semantic chunking...\")\n",
    "semantic_chunks = semantic_chunk(documents)\n",
    "\n",
    "# Save\n",
    "semantic_path = CRAWL_OUT_DIR / \"chunks_semantic.jsonl\"\n",
    "with open(semantic_path, 'w', encoding='utf-8') as f:\n",
    "    for chunk in semantic_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\" Semantic chunking complete: {len(semantic_chunks)} chunks\")\n",
    "print(f\" Saved to: {semantic_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e250ae60",
   "metadata": {},
   "source": [
    "#### 02. Fixed-Window Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aee6ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running fixed-window chunking...\n",
      " Fixed chunking complete: 424 chunks\n",
      " Avg token count: 213.5\n",
      " Saved to: c:\\Development\\real-estate-intelligence-platform\\data\\chunks_fixed.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\" Running fixed-window chunking...\")\n",
    "fixed_chunks = fixed_chunk(documents)\n",
    "\n",
    "# Save\n",
    "fixed_path = CRAWL_OUT_DIR / \"chunks_fixed.jsonl\"\n",
    "with open(fixed_path, 'w', encoding='utf-8') as f:\n",
    "    for chunk in fixed_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "avg_tokens = sum(c['token_count'] for c in fixed_chunks) / len(fixed_chunks) if fixed_chunks else 0\n",
    "print(f\" Fixed chunking complete: {len(fixed_chunks)} chunks\")\n",
    "print(f\" Avg token count: {avg_tokens:.1f}\")\n",
    "print(f\" Saved to: {fixed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f5418",
   "metadata": {},
   "source": [
    "#### 03. Sliding-Window Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1bb931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running sliding-window chunking...\n",
      " Sliding chunking complete: 424 chunks\n",
      " Saved to: c:\\Development\\real-estate-intelligence-platform\\data\\chunks_sliding.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\" Running sliding-window chunking...\")\n",
    "sliding_chunks = sliding_chunk(documents)\n",
    "\n",
    "# Save\n",
    "sliding_path = CRAWL_OUT_DIR / \"chunks_sliding.jsonl\"\n",
    "with open(sliding_path, 'w', encoding='utf-8') as f:\n",
    "    for chunk in sliding_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\" Sliding chunking complete: {len(sliding_chunks)} chunks\")\n",
    "print(f\" Saved to: {sliding_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59befc54",
   "metadata": {},
   "source": [
    "#### 04. Parent-Child Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "787ded37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running Parent Child Chunking...\n",
      " Parent child chunking complete: 848 chunks\n",
      " Avg token count: 213.5\n",
      " Saved to: c:\\Development\\real-estate-intelligence-platform\\data\\chunks_parent_child.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\" Running Parent Child Chunking...\")\n",
    "child_chunks, parent_chunks = parent_child_chunk(documents)\n",
    "parent_child_chunks = child_chunks + parent_chunks\n",
    "\n",
    "# Save\n",
    "parent_child_path = CRAWL_OUT_DIR / \"chunks_parent_child.jsonl\"\n",
    "with open(parent_child_path, 'w', encoding='utf-8') as f:\n",
    "    for chunk in parent_child_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "avg_tokens = sum(c['token_count'] for c in parent_child_chunks) / len(parent_child_chunks) if parent_child_chunks else 0\n",
    "print(f\" Parent child chunking complete: {len(parent_child_chunks)} chunks\")\n",
    "print(f\" Avg token count: {avg_tokens:.1f}\")\n",
    "print(f\" Saved to: {parent_child_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9128ea8e",
   "metadata": {},
   "source": [
    "#### 05. Late Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a86b6e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running Late Chunking...\n",
      "\n",
      " Late chunking complete: 424 base passages\n",
      " Avg token count: 213.5\n",
      " Saved to: c:\\Development\\real-estate-intelligence-platform\\data\\chunks_late.jsonl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" Running Late Chunking...\\n\")\n",
    "late_chunks = late_chunk_index(documents)\n",
    "\n",
    "# Save\n",
    "late_chunks_path = CRAWL_OUT_DIR / \"chunks_late.jsonl\"\n",
    "with open(late_chunks_path, 'w', encoding='utf-8') as f:\n",
    "    for chunk in late_chunks:\n",
    "        f.write(json.dumps(chunk, ensure_ascii=False) + '\\n')\n",
    "\n",
    "avg_tokens = sum(c.get('token_count', 0) for c in late_chunks) / len(late_chunks) if late_chunks else 0\n",
    "print(f\" Late chunking complete: {len(late_chunks)} base passages\")\n",
    "print(f\" Avg token count: {avg_tokens:.1f}\")\n",
    "print(f\" Saved to: {late_chunks_path}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75b553",
   "metadata": {},
   "source": [
    "#### Spot-Check Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b10d450b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Spot-Check: 2 samples from each strategy\n",
      "\n",
      "============================================================\n",
      "SEMANTIC SAMPLES\n",
      "============================================================\n",
      "**Semantic** chunk:\n",
      "  URL: https://www.primelands.lk/land/WOODLAND-ESTATE-IV-KURUNEGALA/en\n",
      "  Strategy: semantic\n",
      "  Text length: 630 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "**Semantic** chunk:\n",
      "  URL: https://www.primelands.lk/close-to-ancient-city-lands/en\n",
      "  Strategy: semantic\n",
      "  Text length: 616 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "============================================================\n",
      "FIXED-WINDOW SAMPLES\n",
      "============================================================\n",
      "**Fixed** chunk:\n",
      "  URL: https://www.primelands.lk/kyc\n",
      "  Strategy: fixed\n",
      "  Text length: 124 chars\n",
      "  Preview: # 404\n",
      "\n",
      "## We are sorry, Page not found!\n",
      "\n",
      "The page you are looking for might have been removed or is ...\n",
      "\n",
      "**Fixed** chunk:\n",
      "  URL: https://www.primelands.lk/land/ELINOR-BATTARAMULLA/en\n",
      "  Strategy: fixed\n",
      "  Text length: 604 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "============================================================\n",
      "SLIDING-WINDOW SAMPLES\n",
      "============================================================\n",
      "**Sliding** chunk:\n",
      "  URL: https://www.primelands.lk/land/AVENLY-KADAWATHA/en\n",
      "  Strategy: sliding\n",
      "  Text length: 598 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "**Sliding** chunk:\n",
      "  URL: https://www.primelands.lk/apartment/GAMPAHA-THE-PALACE/en\n",
      "  Strategy: sliding\n",
      "  Text length: 612 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "============================================================\n",
      "PARENT-CHILD SAMPLES\n",
      "============================================================\n",
      "**Parent-Child** chunk:\n",
      "  URL: https://www.primelands.lk/land/SOUL-CITY-ANURADHAPURA/en\n",
      "  Strategy: child\n",
      "  Text length: 610 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "**Parent-Child** chunk:\n",
      "  URL: https://www.primelands.lk/news/88/Prime-Group-appoints-international-music-icon-Umaria-Sinhawansa-as-Global-Brand-Ambassador/en\n",
      "  Strategy: child\n",
      "  Text length: 752 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "============================================================\n",
      "LATE-CHUNKING SAMPLES\n",
      "============================================================\n",
      "**Late-Chunking** chunk:\n",
      "  URL: https://www.primelands.lk/land/LIGHT-OF-WISDOM-HOMAGAMA/en\n",
      "  Strategy: late_chunk_base\n",
      "  Text length: 614 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n",
      "**Late-Chunking** chunk:\n",
      "  URL: https://www.primelands.lk/house/GAMPAHA-ELEMINT-SUITES/en\n",
      "  Strategy: late_chunk_base\n",
      "  Text length: 612 chars\n",
      "  Preview: [![primelogo.png](https://www.primelands.lk/public/assets/images/primelogo.png)](https://www.primela...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" Spot-Check: 2 samples from each strategy\\n\")\n",
    "\n",
    "def print_sample(chunk, strategy_name):\n",
    "    print(f\"**{strategy_name}** chunk:\")\n",
    "    print(f\"  URL: {chunk['url']}\")\n",
    "    print(f\"  Strategy: {chunk['strategy']}\")\n",
    "    print(f\"  Text length: {len(chunk['text'])} chars\")\n",
    "    print(f\"  Preview: {chunk['text'][:100]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SEMANTIC SAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for chunk in random.sample(semantic_chunks, min(2, len(semantic_chunks))):\n",
    "    print_sample(chunk, \"Semantic\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FIXED-WINDOW SAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for chunk in random.sample(fixed_chunks, min(2, len(fixed_chunks))):\n",
    "    print_sample(chunk, \"Fixed\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SLIDING-WINDOW SAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for chunk in random.sample(sliding_chunks, min(2, len(sliding_chunks))):\n",
    "    print_sample(chunk, \"Sliding\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PARENT-CHILD SAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for chunk in random.sample(parent_child_chunks, min(2, len(child_chunks))):\n",
    "    print_sample(chunk, \"Parent-Child\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LATE-CHUNKING SAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for chunk in random.sample(late_chunks, min(2, len(late_chunks))):\n",
    "    print_sample(chunk, \"Late-Chunking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa52ef56",
   "metadata": {},
   "source": [
    "### 06. Qdrant Indexing\n",
    "Persistent index created using Qdrant. All 5 collections populated with embeddings and rich metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1c6a499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initializing embeddings via OpenRouter...\n",
      " Vector dimension: 3072\n",
      " Initializing persistent Qdrant at: c:\\Development\\real-estate-intelligence-platform\\data\\qdrant_db\n",
      " Collections created.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import uuid\n",
    "import time\n",
    "from context_engineering.config import DATA_DIR, OPENROUTER_BASE_URL, EMBEDDING_MODEL\n",
    "\n",
    "print(\" Initializing embeddings via OpenRouter...\")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    openai_api_key=openrouter_key,\n",
    "    openai_api_base=OPENROUTER_BASE_URL\n",
    ")\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Get vector size from a test embedding\n",
    "sample_vector = embeddings.embed_query(\"Real estate test scenario.\")\n",
    "vector_size = len(sample_vector)\n",
    "print(f\" Vector dimension: {vector_size}\")\n",
    "\n",
    "db_path = str(DATA_DIR / \"qdrant_db\")\n",
    "print(f\" Initializing persistent Qdrant at: {db_path}\")\n",
    "client = QdrantClient(path=db_path)\n",
    "\n",
    "collections = [\n",
    "    \"semantic_chunks\",\n",
    "    \"fixed_chunks\",\n",
    "    \"sliding_chunks\",\n",
    "    \"parent_child_chunks\",\n",
    "    \"late_chunks\"\n",
    "]\n",
    "\n",
    "files = [\n",
    "    \"chunks_semantic.jsonl\",\n",
    "    \"chunks_fixed.jsonl\",\n",
    "    \"chunks_sliding.jsonl\",\n",
    "    \"chunks_parent_child.jsonl\",\n",
    "    \"chunks_late.jsonl\"\n",
    "]\n",
    "\n",
    "# Create collections\n",
    "for col in collections:\n",
    "    if client.collection_exists(col):\n",
    "        client.delete_collection(col)\n",
    "    client.create_collection(\n",
    "        collection_name=col,\n",
    "        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    "    )\n",
    "print(\" Collections created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07eff29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Indexing 414 points into semantic_chunks...\n",
      " ✓ Indexed 2898 points in semantic_chunks\n",
      " Indexing 424 points into fixed_chunks...\n",
      " ✓ Indexed 2968 points in fixed_chunks\n",
      " Indexing 424 points into sliding_chunks...\n",
      " ✓ Indexed 2968 points in sliding_chunks\n",
      " Indexing 848 points into parent_child_chunks...\n",
      " ✓ Indexed 5636 points in parent_child_chunks\n",
      " Indexing 424 points into late_chunks...\n",
      " ✓ Indexed 2168 points in late_chunks\n"
     ]
    }
   ],
   "source": [
    "def index_strategy(collection_name, filename):\n",
    "    filepath = CRAWL_OUT_DIR / filename\n",
    "    if not filepath.exists():\n",
    "        print(f\" File not found: {filename}\")\n",
    "        return\n",
    "        \n",
    "    documents = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            documents.append(json.loads(line))\n",
    "            \n",
    "    print(f\" Indexing {len(documents)} points into {collection_name}...\")\n",
    "    \n",
    "    batch_size = 100\n",
    "    points = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        texts = [doc.get('text', '') for doc in batch]\n",
    "        \n",
    "        try:\n",
    "            batch_embeddings = embeddings.embed_documents(texts)\n",
    "        except Exception as e:\n",
    "            print(f\" Embedding error: {e}\")\n",
    "            continue\n",
    "            \n",
    "        for doc, vector in zip(batch, batch_embeddings):\n",
    "            points.append(\n",
    "                PointStruct(\n",
    "                    id=str(uuid.uuid4()),\n",
    "                    vector=vector,\n",
    "                    payload=doc\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        if len(points) >= 500:\n",
    "            client.upsert(collection_name=collection_name, points=points)\n",
    "            points = []\n",
    "            \n",
    "    if points:\n",
    "        client.upsert(collection_name=collection_name, points=points)\n",
    "        \n",
    "    out_info = client.get_collection(collection_name=collection_name)\n",
    "    print(f\" ✓ Indexed {out_info.points_count} points in {collection_name}\")\n",
    "\n",
    "for col, file in zip(collections, files):\n",
    "    index_strategy(col, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f302854b",
   "metadata": {},
   "source": [
    "### 07. Comparison Metrics\n",
    "Complete comparison table with chunk count, avg token size, index point count, and retrieval time for all strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "369b3d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Measuring retrieval times & building comparison table...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Total Chunks</th>\n",
       "      <th>Avg Chunk Size (Tokens)</th>\n",
       "      <th>Index Size (Points)</th>\n",
       "      <th>Retrieval Time (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic_chunks</td>\n",
       "      <td>414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2898</td>\n",
       "      <td>55.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fixed_chunks</td>\n",
       "      <td>424</td>\n",
       "      <td>213.5</td>\n",
       "      <td>2968</td>\n",
       "      <td>46.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sliding_chunks</td>\n",
       "      <td>424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2968</td>\n",
       "      <td>50.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>parent_child_chunks</td>\n",
       "      <td>848</td>\n",
       "      <td>213.5</td>\n",
       "      <td>5636</td>\n",
       "      <td>102.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>late_chunks</td>\n",
       "      <td>424</td>\n",
       "      <td>213.5</td>\n",
       "      <td>2168</td>\n",
       "      <td>36.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Strategy  Total Chunks  Avg Chunk Size (Tokens)  \\\n",
       "0      semantic_chunks           414                      0.0   \n",
       "1         fixed_chunks           424                    213.5   \n",
       "2       sliding_chunks           424                      0.0   \n",
       "3  parent_child_chunks           848                    213.5   \n",
       "4          late_chunks           424                    213.5   \n",
       "\n",
       "   Index Size (Points)  Retrieval Time (ms)  \n",
       "0                 2898                55.30  \n",
       "1                 2968                46.13  \n",
       "2                 2968                50.86  \n",
       "3                 5636               102.34  \n",
       "4                 2168                36.46  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metrics = []\n",
    "query = \"What are the common practices in property valuation and real estate appraisal?\"\n",
    "query_vector = embeddings.embed_query(query)\n",
    "\n",
    "print(\" Measuring retrieval times & building comparison table...\")\n",
    "\n",
    "for col, file in zip(collections, files):\n",
    "    filepath = CRAWL_OUT_DIR / file\n",
    "    \n",
    "    # 1. Chunk Count & Avg Token Size\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            chunk_count = len(lines)\n",
    "            docs = [json.loads(line) for line in lines]\n",
    "            avg_size = sum(d.get('token_count', 0) for d in docs) / chunk_count if chunk_count > 0 else 0\n",
    "    except Exception:\n",
    "        chunk_count = 0\n",
    "        avg_size = 0.0\n",
    "\n",
    "    # 2. Index Size (Points)\n",
    "    try:\n",
    "        col_info = client.get_collection(collection_name=col)\n",
    "        index_size_points = col_info.points_count\n",
    "    except Exception:\n",
    "        index_size_points = 0\n",
    "        \n",
    "    # 3. Retrieval Time\n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        results = client.query_points(\n",
    "            collection_name=col,\n",
    "            query=query_vector,\n",
    "            limit=5\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "        retrieval_ms = (end_time - start_time) * 1000\n",
    "    except Exception as e:\n",
    "        print(f\"  Retrieval error in {col}: {e}\")\n",
    "        retrieval_ms = 0.0\n",
    "        \n",
    "    metrics.append({\n",
    "        \"Strategy\": col,\n",
    "        \"Total Chunks\": chunk_count,\n",
    "        \"Avg Chunk Size (Tokens)\": round(avg_size, 1),\n",
    "        \"Index Size (Points)\": index_size_points,\n",
    "        \"Retrieval Time (ms)\": round(retrieval_ms, 2)\n",
    "    })\n",
    "\n",
    "# Render Table\n",
    "df = pd.DataFrame(metrics)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454b47a7",
   "metadata": {},
   "source": [
    "### 08. Chunking Strategy Comparison\n",
    "Evaluates 10 real-estate queries across 5 strategies. Measures Precision@5, Recall@5, Answer Relevance, and Latency. \n",
    "Results are saved to `chunking_comparison.csv` and the clear winner is identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b383101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initializing embeddings via OpenRouter...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from context_engineering.config import DATA_DIR, OPENROUTER_BASE_URL, EMBEDDING_MODEL, get_api_key, get_chat_model\n",
    "\n",
    "# Ensure environment is loaded\n",
    "try:\n",
    "    load_dotenv(project_root / \".env\")\n",
    "except:\n",
    "    load_dotenv(\".env\")\n",
    "\n",
    "print(\" Initializing embeddings via OpenRouter...\")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=EMBEDDING_MODEL,\n",
    "    openai_api_key=openrouter_key,\n",
    "    openai_api_base=OPENROUTER_BASE_URL\n",
    ")\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "# client = QdrantClient(path=str(DATA_DIR / \"qdrant_db\"))  # Removed to prevent portalocker unindent lock\n",
    "\n",
    "eval_queries = [\n",
    "    \"What is the valuation process for commercial real estate?\",\n",
    "    \"How does zoning affect land development potential?\",\n",
    "    \"What are the current trends in sustainable building materials for residential homes?\",\n",
    "    \"Explain the role of an escrow agent in a standard property transaction.\",\n",
    "    \"How do interest rates impact mortgage affordability and housing demand?\",\n",
    "    \"What are the primary differences between REITs and direct real estate investment?\",\n",
    "    \"Can you define cap rate and how it is used to assess property performance?\",\n",
    "    \"What are the key clauses to look for in a commercial lease agreement?\",\n",
    "    \"How has the rise of remote work influenced urban vs. suburban real estate markets?\",\n",
    "    \"What are the standard tax implications of selling an investment property (1031 exchange)?\"\n",
    "]\n",
    "\n",
    "collections = [\n",
    "    \"semantic_chunks\",\n",
    "    \"fixed_chunks\",\n",
    "    \"sliding_chunks\",\n",
    "    \"parent_child_chunks\",\n",
    "    \"late_chunks\"\n",
    "]\n",
    "strategies = collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9bf2d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running Evaluation Benchmarks across 10 queries and 5 strategies...\n",
      "\n",
      "Evaluating: semantic_chunks\n",
      " {'id': 'chatcmpl-93a5e812-a49f-4187-bb49-c5f069467ea2', 'object': 'chat.completion', 'created': 1772369073, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.046911952, 'prompt_tokens': 1312, 'prompt_time': 0.002600058, 'completion_tokens': 10, 'completion_time': 0.020999599, 'total_tokens': 1322, 'total_time': 0.023599657, 'prompt_tokens_details': {'cached_tokens': 1280}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_626f3fc5e0', 'x_groq': {'id': 'req_01kjmpvn90fs299bjtq6b96jce', 'seed': 568768335}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-35c26eb3-ef54-45ad-acfd-6edff630d493', 'object': 'chat.completion', 'created': 1772369074, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.04499573, 'prompt_tokens': 1150, 'prompt_time': 0.05503039, 'completion_tokens': 10, 'completion_time': 0.020953082, 'total_tokens': 1160, 'total_time': 0.075983472, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_d29d1d1418', 'x_groq': {'id': 'req_01kjmpvnxgfs3b32c9t0683671', 'seed': 305540396}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-1f9a2889-7e0e-45e8-a3de-14780f32ab53', 'object': 'chat.completion', 'created': 1772369074, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.046218706, 'prompt_tokens': 1435, 'prompt_time': 0.065589594, 'completion_tokens': 10, 'completion_time': 0.020714596, 'total_tokens': 1445, 'total_time': 0.08630419, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_a09bde29de', 'x_groq': {'id': 'req_01kjmpvpmyen09x9tj12sv44gy', 'seed': 1921783095}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-68775934-1a68-4540-b4f5-a85679f97642', 'object': 'chat.completion', 'created': 1772369075, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.04761544, 'prompt_tokens': 1335, 'prompt_time': 0.068694389, 'completion_tokens': 10, 'completion_time': 0.021293705, 'total_tokens': 1345, 'total_time': 0.089988094, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_a09bde29de', 'x_groq': {'id': 'req_01kjmpvqe7e40semep1x64y1t7', 'seed': 283494697}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-f1f528de-f896-4edd-be1d-53355a60c780', 'object': 'chat.completion', 'created': 1772369076, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045277843, 'prompt_tokens': 1273, 'prompt_time': 0.051441802, 'completion_tokens': 10, 'completion_time': 0.020569461, 'total_tokens': 1283, 'total_time': 0.072011263, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_8a618bed98', 'x_groq': {'id': 'req_01kjmpvr3ce42tsdaa0sqg9ph2', 'seed': 1310996310}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-f4f2d0a9-b616-4734-9610-5814d98b589d', 'object': 'chat.completion', 'created': 1772369077, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045599523, 'prompt_tokens': 1347, 'prompt_time': 0.096113367, 'completion_tokens': 10, 'completion_time': 0.020800665, 'total_tokens': 1357, 'total_time': 0.116914032, 'prompt_tokens_details': {'cached_tokens': 1280}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_d29d1d1418', 'x_groq': {'id': 'req_01kjmpvrqden1ty1g4mfyy0d0t', 'seed': 799803237}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-9f06fe0a-eee0-439a-a817-78795e0da60a', 'object': 'chat.completion', 'created': 1772369077, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.046325093, 'prompt_tokens': 1167, 'prompt_time': 0.059674835, 'completion_tokens': 10, 'completion_time': 0.025441503, 'total_tokens': 1177, 'total_time': 0.085116338, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_8a618bed98', 'x_groq': {'id': 'req_01kjmpvsc3e43r816cq401fxzr', 'seed': 1593481955}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-c95a7d9b-19c3-42a3-a42f-d979e1a79a7e', 'object': 'chat.completion', 'created': 1772369078, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.04766076, 'prompt_tokens': 1106, 'prompt_time': 0.062995388, 'completion_tokens': 10, 'completion_time': 0.022990812, 'total_tokens': 1116, 'total_time': 0.0859862, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_626f3fc5e0', 'x_groq': {'id': 'req_01kjmpvt49e44atd4bkk2fdj42', 'seed': 903741379}, 'service_tier': 'on_demand'}\n",
      " {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kh49xa0xfga852wxn5bx5df0` service tier `on_demand` on tokens per minute (TPM): Limit 8000, Used 6934, Requested 1449. Please try again in 2.8725s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " {'id': 'chatcmpl-919e0b45-634e-4ba1-8667-0302550ea7c6', 'object': 'chat.completion', 'created': 1772369139, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.048473132, 'prompt_tokens': 1448, 'prompt_time': 0.074925037, 'completion_tokens': 10, 'completion_time': 0.021054329, 'total_tokens': 1458, 'total_time': 0.095979366, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_a09bde29de', 'x_groq': {'id': 'req_01kjmpxnp3eqhta6zrgew3strm', 'seed': 2074176014}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-84ab283f-f7a8-4e8f-86bd-6e852028c79b', 'object': 'chat.completion', 'created': 1772369140, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045144745, 'prompt_tokens': 1178, 'prompt_time': 0.071351124, 'completion_tokens': 10, 'completion_time': 0.021786639, 'total_tokens': 1188, 'total_time': 0.093137763, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_e10890e4b9', 'x_groq': {'id': 'req_01kjmpxpn4fvnvxvm110rkfky8', 'seed': 1758214620}, 'service_tier': 'on_demand'}\n",
      "\n",
      "Evaluating: fixed_chunks\n",
      " {'id': 'chatcmpl-3a5930d5-672f-4e89-b20a-1fe8a2bdcbf3', 'object': 'chat.completion', 'created': 1772369141, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.04530058, 'prompt_tokens': 1287, 'prompt_time': 0.073936259, 'completion_tokens': 10, 'completion_time': 0.02116735, 'total_tokens': 1297, 'total_time': 0.095103609, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_e10890e4b9', 'x_groq': {'id': 'req_01kjmpxqqze6mbnf6g0zgsv74w', 'seed': 1247383682}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-2c19f002-db46-43e2-92d8-3fcb8cd0b7ae', 'object': 'chat.completion', 'created': 1772369142, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'The task: assess if provided context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.047043075, 'prompt_tokens': 1125, 'prompt_time': 0.057477795, 'completion_tokens': 10, 'completion_time': 0.021151969, 'total_tokens': 1135, 'total_time': 0.078629764, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_626f3fc5e0', 'x_groq': {'id': 'req_01kjmpxrfteqm9psmvyd1rjgnq', 'seed': 50587463}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-b722e8c4-e80e-4998-9e36-8dbb51e0e1df', 'object': 'chat.completion', 'created': 1772369143, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.046011108, 'prompt_tokens': 1410, 'prompt_time': 0.070203501, 'completion_tokens': 10, 'completion_time': 0.020897523, 'total_tokens': 1420, 'total_time': 0.091101024, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_a09bde29de', 'x_groq': {'id': 'req_01kjmpxs9he6nt1e1gy6153g5q', 'seed': 1228773884}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-5b481be9-3f46-458c-a3b6-ce62815a20b3', 'object': 'chat.completion', 'created': 1772369144, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.04560654, 'prompt_tokens': 1310, 'prompt_time': 0.06205803, 'completion_tokens': 10, 'completion_time': 0.021910269, 'total_tokens': 1320, 'total_time': 0.083968299, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_d29d1d1418', 'x_groq': {'id': 'req_01kjmpxt2pfvsrax11qchsyb6t', 'seed': 2065375503}, 'service_tier': 'on_demand'}\n",
      " {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kh49xa0xfga852wxn5bx5df0` service tier `on_demand` on tokens per minute (TPM): Limit 8000, Used 7123, Requested 1249. Please try again in 2.79s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " {'id': 'chatcmpl-c2378f0e-1d71-49f9-b68a-8de56028e170', 'object': 'chat.completion', 'created': 1772369205, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.044835043, 'prompt_tokens': 1248, 'prompt_time': 0.062974074, 'completion_tokens': 10, 'completion_time': 0.020811669, 'total_tokens': 1258, 'total_time': 0.083785743, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_8a618bed98', 'x_groq': {'id': 'req_01kjmpznm8e90vsrkw9f7wqas1', 'seed': 1541005372}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-ecbcf70e-0cd2-4067-b802-8e8a21681a0c', 'object': 'chat.completion', 'created': 1772369206, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045782485, 'prompt_tokens': 1322, 'prompt_time': 0.060098362, 'completion_tokens': 10, 'completion_time': 0.021091822, 'total_tokens': 1332, 'total_time': 0.081190184, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_e10890e4b9', 'x_groq': {'id': 'req_01kjmpzpt0e92vyh2jthpsjqjw', 'seed': 1371450550}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-9191bbd4-e62e-400e-9cb9-ab92a075c1db', 'object': 'chat.completion', 'created': 1772369206, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045842795, 'prompt_tokens': 1142, 'prompt_time': 0.052773331, 'completion_tokens': 10, 'completion_time': 0.020846374, 'total_tokens': 1152, 'total_time': 0.073619705, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_d29d1d1418', 'x_groq': {'id': 'req_01kjmpzqfne938yzf8b91kpvvm', 'seed': 50498964}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-c3a3dd4a-2c24-45d5-bd18-a1567959d1b4', 'object': 'chat.completion', 'created': 1772369207, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'The task: assess if provided context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.044335411, 'prompt_tokens': 1081, 'prompt_time': 0.050131688, 'completion_tokens': 10, 'completion_time': 0.023103766, 'total_tokens': 1091, 'total_time': 0.073235454, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_8a618bed98', 'x_groq': {'id': 'req_01kjmpzr6cet1a64yjxke8m288', 'seed': 1931198342}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-75583483-4f44-4db9-95e3-3a996e73f9cd', 'object': 'chat.completion', 'created': 1772369208, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.04445857, 'prompt_tokens': 1423, 'prompt_time': 0.092688243, 'completion_tokens': 10, 'completion_time': 0.021735441, 'total_tokens': 1433, 'total_time': 0.114423684, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_e10890e4b9', 'x_groq': {'id': 'req_01kjmpzrvbe95vdsezby4bp1t0', 'seed': 555204735}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-c792826c-a9f1-44f3-9dee-6efdc7b7fae4', 'object': 'chat.completion', 'created': 1772369208, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.046547872, 'prompt_tokens': 1153, 'prompt_time': 0.009276867, 'completion_tokens': 10, 'completion_time': 0.020555847, 'total_tokens': 1163, 'total_time': 0.029832714, 'prompt_tokens_details': {'cached_tokens': 1024}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_626f3fc5e0', 'x_groq': {'id': 'req_01kjmpzsj8e96b8q99q7y1qxha', 'seed': 768734172}, 'service_tier': 'on_demand'}\n",
      "\n",
      "Evaluating: sliding_chunks\n",
      " {'id': 'chatcmpl-786adff7-ad03-40ab-9988-1f3df0ef75bf', 'object': 'chat.completion', 'created': 1772369209, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.06236166, 'prompt_tokens': 1287, 'prompt_time': 0.229214838, 'completion_tokens': 10, 'completion_time': 0.023525014, 'total_tokens': 1297, 'total_time': 0.252739852, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_a09bde29de', 'x_groq': {'id': 'req_01kjmpzt69e988hqthnvpxc7m3', 'seed': 1026086952}, 'service_tier': 'on_demand'}\n",
      " {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kh49xa0xfga852wxn5bx5df0` service tier `on_demand` on tokens per minute (TPM): Limit 8000, Used 6966, Requested 1126. Please try again in 690ms. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " {'id': 'chatcmpl-fc95ec97-8719-4153-833f-004bb505cab4', 'object': 'chat.completion', 'created': 1772369270, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'The task: assess if provided context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.156819906, 'prompt_tokens': 1125, 'prompt_time': 0.052068333, 'completion_tokens': 10, 'completion_time': 0.020540565, 'total_tokens': 1135, 'total_time': 0.072608898, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_4727af4560', 'x_groq': {'id': 'req_01kjmq1nwgew1rsh74nmdsa9wa', 'seed': 1845077631}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-c8de4b01-f230-4449-ad0a-b202aeba67f2', 'object': 'chat.completion', 'created': 1772369272, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045915809, 'prompt_tokens': 1410, 'prompt_time': 0.06895032, 'completion_tokens': 10, 'completion_time': 0.020925283, 'total_tokens': 1420, 'total_time': 0.089875603, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_626f3fc5e0', 'x_groq': {'id': 'req_01kjmq1q4nebgv408665cp3zdc', 'seed': 708379421}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-9c185cc0-80b4-4fbc-8b36-72f972078f7c', 'object': 'chat.completion', 'created': 1772369272, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.044544717, 'prompt_tokens': 1310, 'prompt_time': 0.070692032, 'completion_tokens': 10, 'completion_time': 0.020979781, 'total_tokens': 1320, 'total_time': 0.091671813, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_a09bde29de', 'x_groq': {'id': 'req_01kjmq1qxmebjbc1tjfncab64e', 'seed': 1232852235}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-c7b18d34-752c-4e73-a192-b6b657c52e0e', 'object': 'chat.completion', 'created': 1772369273, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.046852328, 'prompt_tokens': 1248, 'prompt_time': 0.059854352, 'completion_tokens': 10, 'completion_time': 0.020543927, 'total_tokens': 1258, 'total_time': 0.080398279, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_626f3fc5e0', 'x_groq': {'id': 'req_01kjmq1rmrew4v56akvz4r9sdk', 'seed': 1878144688}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-48f66453-c568-47bb-ba6d-72e2ca5e99c0', 'object': 'chat.completion', 'created': 1772369274, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045891507, 'prompt_tokens': 1322, 'prompt_time': 0.004468172, 'completion_tokens': 10, 'completion_time': 0.020949745, 'total_tokens': 1332, 'total_time': 0.025417917, 'prompt_tokens_details': {'cached_tokens': 1280}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_e10890e4b9', 'x_groq': {'id': 'req_01kjmq1s8jebkr79d13vf5xczk', 'seed': 1891865343}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-9e5e3516-2788-4df3-8fb2-39efa0a42abd', 'object': 'chat.completion', 'created': 1772369274, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.044770192, 'prompt_tokens': 1142, 'prompt_time': 0.009054348, 'completion_tokens': 10, 'completion_time': 0.026435735, 'total_tokens': 1152, 'total_time': 0.035490083, 'prompt_tokens_details': {'cached_tokens': 1024}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_8a618bed98', 'x_groq': {'id': 'req_01kjmq1swgew69yh5ygszwrd0e', 'seed': 242446738}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-72ce76a5-d56f-4bad-b130-d7090ff2ca69', 'object': 'chat.completion', 'created': 1772369275, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'The task: assess if provided context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.089638665, 'prompt_tokens': 1081, 'prompt_time': 0.089878415, 'completion_tokens': 10, 'completion_time': 0.021720058, 'total_tokens': 1091, 'total_time': 0.111598473, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_a09bde29de', 'x_groq': {'id': 'req_01kjmq1tf3e03tgczwdbrtc7rv', 'seed': 873182329}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-e884d147-3b43-4e2f-a5cb-8f633a891199', 'object': 'chat.completion', 'created': 1772369276, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.044831791, 'prompt_tokens': 1423, 'prompt_time': 0.074024998, 'completion_tokens': 10, 'completion_time': 0.020894237, 'total_tokens': 1433, 'total_time': 0.094919235, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_8a618bed98', 'x_groq': {'id': 'req_01kjmq1v6jebpt2mgf33m8hxr3', 'seed': 917497427}, 'service_tier': 'on_demand'}\n",
      " {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kh49xa0xfga852wxn5bx5df0` service tier `on_demand` on tokens per minute (TPM): Limit 8000, Used 7033, Requested 1154. Please try again in 1.4025s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " {'id': 'chatcmpl-d02cbdbb-7dc0-417e-bab2-24592a24f524', 'object': 'chat.completion', 'created': 1772369337, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.046290901, 'prompt_tokens': 1153, 'prompt_time': 0.054673875, 'completion_tokens': 10, 'completion_time': 0.0246737, 'total_tokens': 1163, 'total_time': 0.079347575, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_d29d1d1418', 'x_groq': {'id': 'req_01kjmq3pn7ee98atxmxh2gpe71', 'seed': 1482257495}, 'service_tier': 'on_demand'}\n",
      "\n",
      "Evaluating: parent_child_chunks\n",
      " {'id': 'chatcmpl-f965a8fb-95a2-4005-a5d1-5676eca23cf3', 'object': 'chat.completion', 'created': 1772369338, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045361831, 'prompt_tokens': 1287, 'prompt_time': 0.000788729, 'completion_tokens': 10, 'completion_time': 0.021535334, 'total_tokens': 1297, 'total_time': 0.022324063, 'prompt_tokens_details': {'cached_tokens': 1280}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_d29d1d1418', 'x_groq': {'id': 'req_01kjmq3qjwe1qbbmqzd55tywbv', 'seed': 2018245746}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-b178f6c2-d68b-4fe3-bce4-9bf0dd2a36a7', 'object': 'chat.completion', 'created': 1772369339, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'The task: assess if provided context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045847588, 'prompt_tokens': 1125, 'prompt_time': 0.036631291, 'completion_tokens': 10, 'completion_time': 0.02958174, 'total_tokens': 1135, 'total_time': 0.066213031, 'prompt_tokens_details': {'cached_tokens': 1024}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_d29d1d1418', 'x_groq': {'id': 'req_01kjmq3rhneyjs6kr4rkcdj4yr', 'seed': 1133906220}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-892b9556-52ed-40e2-b3bd-c5988aa1166b', 'object': 'chat.completion', 'created': 1772369339, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045241101, 'prompt_tokens': 1410, 'prompt_time': 0.068877867, 'completion_tokens': 10, 'completion_time': 0.020977457, 'total_tokens': 1420, 'total_time': 0.089855324, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_8a618bed98', 'x_groq': {'id': 'req_01kjmq3s6eeedbcc97t1cw9e4p', 'seed': 1557569442}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-ea3feec3-e68c-49cc-ba94-675b7c19615a', 'object': 'chat.completion', 'created': 1772369340, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.156551232, 'prompt_tokens': 1310, 'prompt_time': 0.072674147, 'completion_tokens': 10, 'completion_time': 0.020965716, 'total_tokens': 1320, 'total_time': 0.093639863, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_5a93aea882', 'x_groq': {'id': 'req_01kjmq3sx8e1rbwd2a5y9eezab', 'seed': 802438668}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-972dba20-e3f6-468c-8657-8c3d4f42fc71', 'object': 'chat.completion', 'created': 1772369341, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.057292885, 'prompt_tokens': 1248, 'prompt_time': 0.067245194, 'completion_tokens': 10, 'completion_time': 0.022575392, 'total_tokens': 1258, 'total_time': 0.089820586, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_a09bde29de', 'x_groq': {'id': 'req_01kjmq3tt4eymsxvfm6kpt7ysj', 'seed': 310763786}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-469ac59e-994a-4798-b794-092ce337e898', 'object': 'chat.completion', 'created': 1772369342, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.047175884, 'prompt_tokens': 1322, 'prompt_time': 0.069065785, 'completion_tokens': 10, 'completion_time': 0.029170686, 'total_tokens': 1332, 'total_time': 0.098236471, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_626f3fc5e0', 'x_groq': {'id': 'req_01kjmq3vgte1stty51fcdsmt9y', 'seed': 1106875142}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-4b29ba1f-7cc2-4c2d-9bf1-7be5718a4e95', 'object': 'chat.completion', 'created': 1772369342, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.046784057, 'prompt_tokens': 1142, 'prompt_time': 0.053158433, 'completion_tokens': 10, 'completion_time': 0.022304181, 'total_tokens': 1152, 'total_time': 0.075462614, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_626f3fc5e0', 'x_groq': {'id': 'req_01kjmq3wa4eynra9pvfgpz204t', 'seed': 934187436}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-65f32153-fd15-4120-8d48-fd4344200857', 'object': 'chat.completion', 'created': 1772369343, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'The task: assess if provided context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.156893543, 'prompt_tokens': 1081, 'prompt_time': 0.045549666, 'completion_tokens': 10, 'completion_time': 0.02078931, 'total_tokens': 1091, 'total_time': 0.066338976, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_4727af4560', 'x_groq': {'id': 'req_01kjmq3x13eypbpjd0zt4k2vp7', 'seed': 1024838183}, 'service_tier': 'on_demand'}\n",
      " {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kh49xa0xfga852wxn5bx5df0` service tier `on_demand` on tokens per minute (TPM): Limit 8000, Used 7892, Requested 1424. Please try again in 9.87s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      " {'id': 'chatcmpl-0f810bbf-a849-4c53-9c9f-b0126525e46d', 'object': 'chat.completion', 'created': 1772369404, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045655827, 'prompt_tokens': 1423, 'prompt_time': 0.011842222, 'completion_tokens': 10, 'completion_time': 0.020570888, 'total_tokens': 1433, 'total_time': 0.03241311, 'prompt_tokens_details': {'cached_tokens': 1280}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_d29d1d1418', 'x_groq': {'id': 'req_01kjmq5rraegy8xfnpxp6p9y86', 'seed': 1562983305}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-820c53ed-3361-4f31-b3fd-ef3b48cef7f8', 'object': 'chat.completion', 'created': 1772369405, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.046651725, 'prompt_tokens': 1153, 'prompt_time': 0.081691514, 'completion_tokens': 10, 'completion_time': 0.021274876, 'total_tokens': 1163, 'total_time': 0.10296639, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_a09bde29de', 'x_groq': {'id': 'req_01kjmq5stye3q8wq8gpt4kdta3', 'seed': 1260609429}, 'service_tier': 'on_demand'}\n",
      "\n",
      "Evaluating: late_chunks\n",
      " {'id': 'chatcmpl-8ee61dd9-caac-4207-81a5-0cf5a33fb664', 'object': 'chat.completion', 'created': 1772369406, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.046986745, 'prompt_tokens': 1287, 'prompt_time': 0.001768235, 'completion_tokens': 10, 'completion_time': 0.02182809, 'total_tokens': 1297, 'total_time': 0.023596325, 'prompt_tokens_details': {'cached_tokens': 1280}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_d29d1d1418', 'x_groq': {'id': 'req_01kjmq5thze3sr1hjkvdt2ky5v', 'seed': 2130741688}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-30015c82-b4d2-464c-a09b-41899dd2e158', 'object': 'chat.completion', 'created': 1772369407, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'The task: assess if provided context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.056955731, 'prompt_tokens': 1125, 'prompt_time': 0.153302069, 'completion_tokens': 10, 'completion_time': 0.022217918, 'total_tokens': 1135, 'total_time': 0.175519987, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_a09bde29de', 'x_groq': {'id': 'req_01kjmq5v6ye3srt67a8ady2qjk', 'seed': 1982299740}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-58b3095b-d760-442b-8f18-ee628e1180f8', 'object': 'chat.completion', 'created': 1772369408, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045992978, 'prompt_tokens': 1410, 'prompt_time': 0.075734522, 'completion_tokens': 10, 'completion_time': 0.02122671, 'total_tokens': 1420, 'total_time': 0.096961232, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_e10890e4b9', 'x_groq': {'id': 'req_01kjmq5w1ce3t9efagq5zx1ve3', 'seed': 1294471440}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-2ff228b6-d960-4d3c-975b-0f7c9f66e125', 'object': 'chat.completion', 'created': 1772369409, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045937375, 'prompt_tokens': 1310, 'prompt_time': 1.071391309, 'completion_tokens': 10, 'completion_time': 0.020586458, 'total_tokens': 1320, 'total_time': 1.091977767, 'prompt_tokens_details': {'cached_tokens': 1280}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_d29d1d1418', 'x_groq': {'id': 'req_01kjmq5wnveh38rj17qryd9wsn', 'seed': 361626431}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-8b149e0c-27a6-49a3-87a4-3ea3cba5aae5', 'object': 'chat.completion', 'created': 1772369411, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045978433, 'prompt_tokens': 1248, 'prompt_time': 0.503726568, 'completion_tokens': 10, 'completion_time': 0.020617438, 'total_tokens': 1258, 'total_time': 0.524344006, 'prompt_tokens_details': {'cached_tokens': 1024}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_d29d1d1418', 'x_groq': {'id': 'req_01kjmq5yndeh4vkh2h54ajrzc4', 'seed': 223104456}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-cdc015c6-dab4-4168-bdd5-fe6761cd083d', 'object': 'chat.completion', 'created': 1772369412, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.15686653, 'prompt_tokens': 1322, 'prompt_time': 0.183166239, 'completion_tokens': 10, 'completion_time': 0.0207274, 'total_tokens': 1332, 'total_time': 0.203893639, 'prompt_tokens_details': {'cached_tokens': 1280}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_5a93aea882', 'x_groq': {'id': 'req_01kjmq5ztmf0s94wkqhm4ww91w', 'seed': 2081368756}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-baa0667e-f943-4bd6-832d-65b494a7e49c', 'object': 'chat.completion', 'created': 1772369412, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'The task: evaluate if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045459242, 'prompt_tokens': 1142, 'prompt_time': 0.053596167, 'completion_tokens': 10, 'completion_time': 0.020932396, 'total_tokens': 1152, 'total_time': 0.074528563, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_e10890e4b9', 'x_groq': {'id': 'req_01kjmq60pze428ktp6qf6grax3', 'seed': 1985951082}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-110616aa-1bba-4c18-b8c0-22a056ccdf26', 'object': 'chat.completion', 'created': 1772369413, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'The task: assess if provided context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.046717169, 'prompt_tokens': 1081, 'prompt_time': 0.005972821, 'completion_tokens': 10, 'completion_time': 0.020605684, 'total_tokens': 1091, 'total_time': 0.026578505, 'prompt_tokens_details': {'cached_tokens': 1024}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_626f3fc5e0', 'x_groq': {'id': 'req_01kjmq61awf0vr4nxeje28z896', 'seed': 1940750074}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-36871a07-b33c-484d-b49d-c7ccbc5cc444', 'object': 'chat.completion', 'created': 1772369415, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the provided'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045203344, 'prompt_tokens': 1423, 'prompt_time': 1.442610702, 'completion_tokens': 10, 'completion_time': 0.024959616, 'total_tokens': 1433, 'total_time': 1.467570318, 'prompt_tokens_details': {'cached_tokens': 1280}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_e10890e4b9', 'x_groq': {'id': 'req_01kjmq61zne44begh80fnm6499', 'seed': 374463060}, 'service_tier': 'on_demand'}\n",
      " {'id': 'chatcmpl-2a640d94-f55f-4076-80b0-1afd5036496a', 'object': 'chat.completion', 'created': 1772369416, 'model': 'openai/gpt-oss-120b', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '', 'reasoning': 'We need to assess if the context'}, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'queue_time': 0.045801472, 'prompt_tokens': 1153, 'prompt_time': 0.037727718, 'completion_tokens': 10, 'completion_time': 0.020776325, 'total_tokens': 1163, 'total_time': 0.058504043, 'prompt_tokens_details': {'cached_tokens': 1024}, 'completion_tokens_details': {'reasoning_tokens': 8}}, 'usage_breakdown': None, 'system_fingerprint': 'fp_a09bde29de', 'x_groq': {'id': 'req_01kjmq6453f0zrv3k528mxv7ce', 'seed': 320190277}, 'service_tier': 'on_demand'}\n",
      "\n",
      "==================================================\n",
      "\n",
      " CLEAR WINNER: late_chunks\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Precision@5</th>\n",
       "      <th>Recall@5</th>\n",
       "      <th>Answer Relevance</th>\n",
       "      <th>Latency (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>late_chunks</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fixed_chunks</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sliding_chunks</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic_chunks</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>parent_child_chunks</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Strategy  Precision@5  Recall@5  Answer Relevance  Latency (ms)\n",
       "4          late_chunks          0.0       0.0               0.0          28.6\n",
       "1         fixed_chunks          0.0       0.0               0.0          36.6\n",
       "2       sliding_chunks          0.0       0.0               0.0          37.2\n",
       "0      semantic_chunks          0.0       0.0               0.0          37.5\n",
       "3  parent_child_chunks          0.0       0.0               0.0          73.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: c:\\Development\\real-estate-intelligence-platform\\data\\chunking_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "def evaluate_relevance(query, retrieved_texts):\n",
    "    \"\"\"LLM-as-a-judge to determine if retrieved texts contain the answer. Returns score 0.0-1.0\"\"\"\n",
    "    if not retrieved_texts:\n",
    "        return 0.0\n",
    "        \n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_texts)\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert evaluator. Assess if the provided context contains the answer to the query.\n",
    "    Query: {query}\n",
    "    Context: {context}\n",
    "    \n",
    "    Score from 0 (completely irrelevant) to 10 (perfectly answers the query). Output ONLY the number.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_retries = 6\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            groq_key = get_api_key(\"groq\")\n",
    "            groq_model = get_chat_model(provider=\"groq\", tier=\"reason\")\n",
    "            \n",
    "            response = requests.post(\n",
    "                \"https://api.groq.com/openai/v1/chat/completions\",\n",
    "                headers={\"Authorization\": f\"Bearer {groq_key}\"},\n",
    "                json={\n",
    "                    \"model\": groq_model,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"temperature\": 0,\n",
    "                    \"max_tokens\": 10\n",
    "                }\n",
    "            )\n",
    "            data = response.json()\n",
    "            print(f\" {data}\")\n",
    "            if \"error\" in data and data[\"error\"].get(\"code\") == \"rate_limit_exceeded\":\n",
    "                # wait_time = 3 + (attempt * 2)\n",
    "                wait_time = 60\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "                \n",
    "            score_text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"0\").strip()\n",
    "            if score_text == \"0\" and \"error\" in data:\n",
    "                print(f\" API Error: {data['error']}\")\n",
    "                return 0.0\n",
    "                \n",
    "            match = re.search(r\"\\d+\", score_text)\n",
    "            score = float(match.group()) if match else 0.0\n",
    "            return min(score / 10.0, 1.0)\n",
    "        except Exception as e:\n",
    "            print(f\" Eval error: {e}\")\n",
    "            time.sleep(2)\n",
    "    return 0.0\n",
    "\n",
    "results_data = []\n",
    "print(f\"\\n Running Evaluation Benchmarks across {len(eval_queries)} queries and {len(strategies)} strategies...\")\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\nEvaluating: {strategy}\")\n",
    "    strategy_metrics = {\n",
    "        \"Strategy\": strategy,\n",
    "        \"Precision@5\": [],\n",
    "        \"Recall@5\": [],\n",
    "        \"Answer Relevance\": [],\n",
    "        \"Latency (ms)\": []\n",
    "    }\n",
    "    \n",
    "    for q_idx, query in enumerate(eval_queries):\n",
    "        query_vector = embeddings.embed_query(query)\n",
    "        \n",
    "        try:\n",
    "            start = time.perf_counter()\n",
    "            hits = client.query_points(\n",
    "                collection_name=strategy,\n",
    "                query=query_vector,\n",
    "                limit=5\n",
    "            ).points\n",
    "            latency = (time.perf_counter() - start) * 1000\n",
    "        except Exception as e:\n",
    "            print(f\"Retrieval failed for {strategy}: {e}\")\n",
    "            hits = []\n",
    "            latency = 0\n",
    "            \n",
    "        strategy_metrics[\"Latency (ms)\"].append(latency)\n",
    "        \n",
    "        if not hits:\n",
    "            strategy_metrics[\"Precision@5\"].append(0.0)\n",
    "            strategy_metrics[\"Recall@5\"].append(0.0)\n",
    "            strategy_metrics[\"Answer Relevance\"].append(0.0)\n",
    "            continue\n",
    "            \n",
    "        retrieved_texts = [hit.payload.get('text', '') for hit in hits if hit.payload]\n",
    "        \n",
    "        relevance_score = evaluate_relevance(query, retrieved_texts)\n",
    "        strategy_metrics[\"Answer Relevance\"].append(relevance_score)\n",
    "        \n",
    "        is_hit = relevance_score >= 0.7\n",
    "        strategy_metrics[\"Precision@5\"].append(1.0 if is_hit else 0.0)\n",
    "        strategy_metrics[\"Recall@5\"].append(1.0 if is_hit else 0.0)\n",
    "        \n",
    "    results_data.append({\n",
    "        \"Strategy\": strategy,\n",
    "        \"Precision@5\": round(np.mean(strategy_metrics[\"Precision@5\"]), 3),\n",
    "        \"Recall@5\": round(np.mean(strategy_metrics[\"Recall@5\"]), 3),\n",
    "        \"Answer Relevance\": round(np.mean(strategy_metrics[\"Answer Relevance\"]), 3),\n",
    "        \"Latency (ms)\": round(np.mean(strategy_metrics[\"Latency (ms)\"]), 1)\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(results_data)\n",
    "eval_df = eval_df.sort_values(by=[\"Answer Relevance\", \"Latency (ms)\"], ascending=[False, True])\n",
    "winner = eval_df.iloc[0]['Strategy']\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"\\n CLEAR WINNER: {winner}\\n\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "display(eval_df)\n",
    "\n",
    "try:\n",
    "    csv_path = CRAWL_OUT_DIR / \"chunking_comparison.csv\"\n",
    "    eval_df.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nResults saved to: {csv_path}\")\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sahas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
